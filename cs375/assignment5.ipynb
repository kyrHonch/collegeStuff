{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tajfsk_7JY3E"
   },
   "source": [
    "**Note to grader:** Each question is assigned with a score. The final score will be (sum of actual scores)/(sum of maximum scores)*100. The grading rubrics are shown in the TA guidelines."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Grader's area\n",
    "import numpy as np\n",
    "\n",
    "maxScore = 0\n"
   ],
   "metadata": {
    "id": "zPnHTf9MfT5X",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NnOFlqPI1qn"
   },
   "source": [
    "# **Assignment 5**\n",
    "<br>\n",
    "\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zSyX_FIzI1qn"
   },
   "source": [
    "from IPython.display import Math\n",
    "from IPython.display import Latex\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import struct\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "----------------\n",
    "----------------\n",
    "----------------"
   ],
   "metadata": {
    "id": "cc6kqRYyliiK"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXayK-GxI1qo"
   },
   "source": [
    "## <font color='blue'> Question 0. MLP with scikit-learn: Explore outputs for the MNIST dataset [40 pts]</font>\n",
    "\n",
    "The purpose of this question is to get some practice time with MLP classifiers. We will be using the the MNIST data set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xscTgg2dI1qo",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7eda0b76-c7ce-4933-c18e-60f4583f055f"
   },
   "source": [
    "# Let's first load the MNIST data set.\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "y = y.astype(int)\n",
    "X = ((X / 255.) - .5) * 2\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=10000, random_state=123, stratify=y)\n"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYAL83hpI1qp"
   },
   "source": [
    "<br> **Q0-0**: This is the classifier we built in class. Find a setting for *batch_size* and *max_iter*, so that the subsequent fit function **converges**. Converge here is defined as: the training loss does not improve by more than tol=1e-4 for n_iter_no_change=10 consecutive passes over the training set. [10 pts]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6I39VdCmI1qp"
   },
   "source": [
    "\n",
    "mlp_mnist = MLPClassifier(verbose = True, hidden_layer_sizes=(50,), batch_size = 5, \\\n",
    "                          max_iter=50, solver='sgd', activation='logistic',\\\n",
    "                          learning_rate = 'constant', learning_rate_init = 0.001, random_state=1)\n",
    "\n"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Hv4XQEb-I1qp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3d00d750-2cdd-4901-91e7-3ec68f5d32dc"
   },
   "source": [
    "mlp_mnist.fit(X_train,y_train)\n",
    "#"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteration 1, loss = 0.57329662\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mDpqVYN1I1qq"
   },
   "source": [
    "predictions = mlp_mnist.predict(X_test)\n",
    "probability_predictions = mlp_mnist.predict_proba(X_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaKRY5l7I1qq"
   },
   "source": [
    "**Q0-1**: Write a short function that outputs the **second best** prediction for each point, i.e. the label which gets the second highest probability in the softmax output. [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d3ZqtfJTI1qq"
   },
   "source": [
    "### Your code here"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7bbDT1CI1qq"
   },
   "source": [
    "**Q0-2**: For each data point misclassified by the model, find if the second best prediction is actually the correct label. Calculate the percentage of missclassified points, that are correctly classified based on the second best prediction. [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CVEQ7pLPI1qq"
   },
   "source": [
    "### Your code here. Also give some comments about your findings\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPkG0fb2I1qq"
   },
   "source": [
    "**Q0-3**: From a human point of view, we can perhaps see a frequent similarity between handritten digits 3 and 5 (i.e. sometimes we have to pay closer attention to figure out what the true written digit is). We may want to ask if the MLP also detects digit similarities.  \n",
    "<br>\n",
    " Consider then the subset $X_i$ of data points that have label $i$ (e.g. all data points with label 5). For each label $j\\neq i$, report the frequency by which label $j$ (e.g. label 3) shows up as second best prediction for points in $X_i$. [10 pts]\n",
    "<br>\n",
    "<br>\n",
    "***Further Clarification***: Suppose we have 100 data points with label i=5. Let's say that for 28 of these 100 points, the second predicted label is j=3. Then, the frequency of j=3 as second-best prediction to i=5 is 28%.Â  We want to compute that frequency for every i and j.\n",
    "\n",
    "Example output:\n",
    "> i = 0: 986 (9.86%)\n",
    "\n",
    "  j = 1: 2 (0.203%)\n",
    "\n",
    "  j = 2: 248 (25.152%)\n",
    "\n",
    "  j = 3: 8 (0.811%)\n",
    "\n",
    "  j = 4: 4 (0.406%)\n",
    "\n",
    "  j = 5: 270 (27.383%)\n",
    "\n",
    "  j = 6: 188 (19.067%)\n",
    "\n",
    "  j = 7: 56 (5.68%)\n",
    "\n",
    "  j = 8: 52 (5.274%)\n",
    "\n",
    "  j = 9: 150 (15.213%)\n",
    "\n",
    "<br>  \n",
    " Give your code and comment on your findings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZL_sboa1I1qq"
   },
   "source": [
    "### Your code here. Also give some comments about your findings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IllLoXxGAIIo"
   },
   "source": [
    "# For grader use only\n",
    "\n",
    "\n",
    "\n",
    "maxScore = maxScore + 40\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "------------------------------\n",
    "------------------------------\n",
    "------------------------------"
   ],
   "metadata": {
    "id": "oko-ZnXEpBMc"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bescj6mdI1qq"
   },
   "source": [
    "## <font color='blue'>Question 1: Working with warm start for focused training [10 pts by 4 = 40 pts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmDZTC1UI1qq"
   },
   "source": [
    "**Q1-0**. MLP classifiers in scikit-learn have an option for so-called 'warm start'. Do a bit of research to find out what it is. Then:\n",
    "\n",
    "Give an example of warm start. Specifically, in scenario 1:\n",
    "train the network for $k$ epochs (max_iter=k), and then without using warm start\n",
    "continue the training for another $k$ epochs; in scenario 2: train the network for $k$ epochs, and then using warm start\n",
    "continue the training for another $k$ epochs. Check the outputs from training, you will find the different between two scenarios.\n",
    "Here $k$ is a parameter of your choice, you can set $k=5$. [10 points]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8wY3lNEgI1qq"
   },
   "source": [
    "### Your code here. Also give some comments about your findings\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwK_llxsI1qq"
   },
   "source": [
    "\n",
    "The training set usally remains the same after a warm start. Here, we will use a very similar bit not identical dataset after the warm start:\n",
    "<br>\n",
    "<br>\n",
    "**Q1-1**. Randomly divide the MNIST dataset into three part: train1 (50000), train2(10000), test (10000). Make sure that all number classes are presented in both train1, and train2. Hints: use train_test_split(). [10 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DwlNHIdcI1qq"
   },
   "source": [
    "### Your code here. Also give some comments about your findings\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qj639NWSI1qq"
   },
   "source": [
    "**Q1-2**. Use warm start and train your model using the train1 dataset for $2k$ epochs. Then, using warm start, continute the training with train2 dataset for another $2k$ epochs. [10 points]\n",
    "<br>\n",
    "<br>\n",
    "Here $k$ is the same parameter you used above. Notice that each point\n",
    "is considered $2k$ times in this training, exactly as in part 0.\n",
    "<br>\n",
    "<br>\n",
    "In effect this process of training tries spend more time learning half of the training set, and then moves to the second part of the training set to spend equal time.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qrr0rwxrI1qq"
   },
   "source": [
    "### Your code here. Also give some comments about your findings\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVOatk-gVU4O"
   },
   "source": [
    "**Q1-3**. Which of the two above strategies\n",
    "works better? (Q1-0 or Q1-2). Feel free to experiment and report your findings. [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5r9YkG3xsS5l"
   },
   "source": [
    "# For grader use only\n",
    "\n",
    "maxScore = maxScore + 40\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------------------------\n",
    "--------------------------\n",
    "--------------------------"
   ],
   "metadata": {
    "id": "AgUw1wklsZOi"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBaZ0tdBVvDT"
   },
   "source": [
    "##  <font color='blue'> Q2. How Powerful are Neural Networks? [20 pts]\n",
    "\n",
    "There is a lot of hype around Neural Networks, and their ability to solver problems. This is of course justified, but we should always be aware of their limitations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrCrmP4tWnGx"
   },
   "source": [
    "Recall that every row in the matrix $X$  above comes from an image. Every entry of a row corresponds to a (grayscale) pixel. In particular, the first colum of $X$ contains the grayscale values of the first pixels of the images in the data set.\n",
    "<br>\n",
    "<br>\n",
    "We will now 'hide' the label of the image, inside the image itself.\n",
    "Here is one way to do that: Replace the top-left pixel of the image (which is a value from 0 to 255) with the label for that image. For example, if the image\n",
    "contains number 3, then the first pixel of the image will now be set to 3. (In fact, let's set this to 3/255 because we already divided all pixel values by 255)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dI8SbkQ2Xh6b"
   },
   "source": [
    "**Q2-0.** Create a new dataset $X1$, where each row is the same as the corresponding row of $X$, except in the first column, which contains the label of the corresponding image, divided by 255. Then split\n",
    "the set $X1$ into a training set and a test set, as we did with $X$. [10 pts]\n",
    "\n",
    "Hints: make a copy of X and work on the copy data: X1 = X.copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nxGaaYSbZc3y"
   },
   "source": [
    "# your code here\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDmOC16kZgqM"
   },
   "source": [
    "**Q2-1.** With the label of the image 'planted' in the image itself, a Neural Network can achieve accuracy 100%. This is because all the weights connecting to pixels $2...28^2$ can be set to 0, and so effectively the only feature that will be taken into account is the first pixel. The question is whether an MLP can indeed 'discover' this weighting via training. [10 pts]\n",
    "<br>\n",
    "<br>\n",
    "Use an MLP classifier  with one hidden layer of any size you want. Train it on $X1$ and report the accuracy. Does it reach 100\\%? Is the accuracy better relative to training with the original input $X$?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fydXo8GRGkbp"
   },
   "source": [
    "# for grader use\n",
    "\n",
    "# Total Grade Calculation\n",
    "\n",
    "maxScore = maxScore + 20\n",
    "score = actualScore*100/maxScore"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
